{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "from numpy.random import multivariate_normal\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import mixture\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons,make_blobs\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of helper functions we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_2d_gaussian(meanx,meany,variance_x,variance_y,covariance,numsamps):\n",
    "    '''\n",
    "    Generates a random sample of size 'numsamps' from a 2-dimensional Gaussian distribution.\n",
    "    The Gaussian is defined by the mean vector (meanx,meany) and the \n",
    "    covariance matrix\n",
    "    \n",
    "    variance_x    covariance\n",
    "    covariance    variance_y\n",
    "    \n",
    "    All parameters can be freely chosen, except covariance, which must satisfy the inequality\n",
    "    \n",
    "    covariance <= sqrt(variance_x * variance_y)\n",
    "    '''\n",
    "    meanvec = np.array([meanx,meany])\n",
    "    covarmatrix = np.array([[variance_x,covariance],[covariance,variance_y]])\n",
    "    return multivariate_normal(meanvec,covarmatrix,numsamps)\n",
    "\n",
    "\n",
    "def maxpos(A):\n",
    "    '''\n",
    "    Takes an n x k array A, and returns 1-dim n array where the i'th\n",
    "    entry is the index of column in A where the i'th row of A has its\n",
    "    maximal value (application: turns a probabilitiy distribution over\n",
    "    k classes for n instances into a single prediction)\n",
    "    '''\n",
    "    return np.argmax(A,axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Decision regions for simple 2 dimensional datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading/generating data: following are several alternative ways to get data consisting of the input features stored in 'features', and the class labels, stored in 'labels'.  Only execute one of the data generating cells, before moving on to the following code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: sampling data from 2-dimensional Gaussian distributions. For reproducibility, one can fix a random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "datasize=250\n",
    "mixturecoeff=np.array([0.4,0.2,0.4])\n",
    "componentsizes=(datasize*mixturecoeff).astype(int)\n",
    "\n",
    "# Original\n",
    "class0samp=sample_2d_gaussian(2,3,0.5,0.5,0.45,componentsizes[0])\n",
    "class1samp=sample_2d_gaussian(5,3,1.0,0.5,-0.45,componentsizes[1])\n",
    "class2samp=sample_2d_gaussian(3,2,0.5,0.5,0,componentsizes[2])\n",
    "  \n",
    "# class0samp=sample_2d_gaussian(5,4,0.5,0.5,0.45,componentsizes[0])\n",
    "# class1samp=sample_2d_gaussian(5,3,1.0,0.5,-0.45,componentsizes[1])\n",
    "# class2samp=sample_2d_gaussian(5,2,0.5,0.5,0,componentsizes[2])\n",
    "  \n",
    "  \n",
    "features=np.concatenate((class0samp,class1samp,class2samp),axis=0)\n",
    "labels=np.concatenate((np.zeros(componentsizes[0]),np.ones(componentsizes[1]),2*np.ones(componentsizes[2])))         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B**: Loading mi.txt data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# midata = pd.read_csv(\"MI-labeled.txt\", sep=',')\n",
    "# features = np.array(midata[['X1','X2']])\n",
    "# classlabels=midata['Class']\n",
    "# labels = np.zeros(len(classlabels))\n",
    "# for i in range(len(classlabels)):\n",
    "#     if classlabels[i] == 'I':\n",
    "#         labels[i]=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the data with labels indicated by color:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(features[:,0],features[:,1],c=labels, cmap = mpl.colors.ListedColormap(['r', 'b','g']))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing meshgrid for plotting decision regions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxvalx = np.max(features[:,0])\n",
    "maxvaly = np.max(features[:,1])\n",
    "minvalx = np.min(features[:,0])\n",
    "minvaly = np.min(features[:,1])\n",
    "border=2\n",
    "xinterval = (maxvalx-minvalx)/border\n",
    "yinterval = (maxvaly-minvaly)/border\n",
    "xx, yy = np.meshgrid(np.arange(minvalx-xinterval, maxvalx+xinterval, xinterval/100), np.arange(minvaly-yinterval, maxvaly+yinterval, yinterval/100))\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning a classifier -- uncomment to select the classification model to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model=LinearDiscriminantAnalysis()\n",
    "#model = LogisticRegression()\n",
    "model = SVC(kernel='linear')\n",
    "\n",
    "model.fit(features,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying model to the meshgrid. All models return a quantitative \"likelihood\" for the different classes. For the probabilistic models, these are class label probabilities that are retrieved using the `.predict_proba` method. For the non-probabilistic SVC model, this is the `decision_function` method. In all cases, we classify a datapoint as belonging to the class with the maximal \"likelihood\" value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Z=model.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z=model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "meshclasses = maxpos(Z)\n",
    "#meshclasses = np.ones(Z.size)\n",
    "#meshclasses[Z<0]=0\n",
    "meshclasses = meshclasses.reshape(xx.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting datapoints and decision regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contourf(xx,yy,meshclasses,[-0.1,0.1,1,2],colors=('tomato','lightblue','lightgreen'))\n",
    "plt.scatter(features[:,0],features[:,1],c=labels, cmap = mpl.colors.ListedColormap(['r', 'b','g']))\n",
    "#plt.scatter(data[:,0],data[:,1],c=classlabels_numeric, cmap = mpl.colors.ListedColormap(['r', 'b']))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also calculate the predictions on the (training) datapoints, and check the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels=model.predict(features)\n",
    "print(\"Accuracy: {}\".format(accuracy_score(labels,pred_labels)))\n",
    "\n",
    "#LDA: 0.896 (3 -> 0.908)\n",
    "#Logistic: 0.956 (3 -> 0.924)\n",
    "#SVC: 0.956 (3 -> 0.936)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1**: Experiment with the three classifiers Linear Discriminant Analysis, Logistic Regression, and linear support vector machines. Use the MI data, and different mixtures of Gaussians you create by varying the parameters in `sample_2d_gaussian'. \n",
    "\n",
    "For the Gaussian mixture data:\n",
    "\n",
    "Create one or several linearly separable Gaussian mixture datasets (can be with only two components) for which some of the models give a perfect separation of the classes, and others don't (this is related to Exercise 4 in self study 1). \n",
    "\n",
    "Create separate test sets using the same parameters for the Gaussian mixture as in the creation of the training set. Can you create a test/train dataset and find classifiers A and B, such that A is better than B on the training data, but B is better than A on the test set? Why is this not such an easy task for the datasets and classifiers we consider her?\n",
    "\n",
    "For the MI data:\n",
    "\n",
    "explain the structure of the decision regions you find for the different classifiers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: the California Housing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next investigate some \"real\" data: the California housing dataset contains attributes characterizing houses in different \"blocks\" of California, and the median house value in these blocks. The usual machine learning problem for this dataset is the regression problem of predicting the median value. We can turn it into a binary prediction problem whether the median value is above the median (of the median values). \n",
    "\n",
    "The `fetch_california_housing` function returns the dataset in the form of a dictionary with attributes DESCR, data, and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "california_housing = fetch_california_housing()\n",
    "\n",
    "\n",
    "print(california_housing['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a first illustration, we plot the datapoints by geographic coordinates with color coded median house prices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(california_housing['data'][:,7],california_housing['data'][:,6],s=1,c=california_housing['target'])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate our binary labels, and create a train/test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array([1 if y > np.median(california_housing['target']) else 0 for y in california_housing['target']])\n",
    "features = california_housing['data']\n",
    "features_train,features_test,labels_train,labels_test = train_test_split(features,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this discretization the data looks as follows (showing the full data, not train or test splits):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(california_housing['data'][:,7],california_housing['data'][:,6],s=1,c=labels)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can directly see that the coordinates are already pretty good predictors, though clearly not allowing for linear separbility. We investigate how good a model we get by using all features and different model classes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment one of the following:\n",
    "\n",
    "#model=LinearDiscriminantAnalysis()\n",
    "#model = LogisticRegression()\n",
    "model = SVC(kernel='linear')\n",
    "\n",
    "model.fit(features_train,labels_train)\n",
    "pred_labels_train=model.predict(features_train)\n",
    "pred_labels_test=model.predict(features_test)\n",
    "\n",
    "print(\"Accuracy train: {}\".format(accuracy_score(labels_train,pred_labels_train)))\n",
    "print(\"Accuracy test: {}\".format(accuracy_score(labels_test,pred_labels_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**: Try the different models on the California housing data. Which one is doing best in terms of test accuracy?\n",
    "\n",
    "Inspect the following attributes of the learned models:\n",
    "\n",
    "\n",
    "LDA: `means_` <br>\n",
    "Logistic regression: `coef_` <br>\n",
    "SVC: `coef_`<br>\n",
    "\n",
    "Check the online documentation about the meaning of these attributes. How can the values of these attributes be used for explaining the classification model? Which of the input features are most important for the prediction? \n",
    "\n",
    "For this investigation it may be helpful to normalize the data before building the classifiers, using the code in the cell below (why is this useful?).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(features_train)\n",
    "features_train_norm=scaler.transform(features_train)\n",
    "features_test_norm=scaler.transform(features_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment one of the following:\n",
    "\n",
    "# # model=LinearDiscriminantAnalysis()\n",
    "# #model = LogisticRegression()\n",
    "# model = SVC(kernel='linear')\n",
    "\n",
    "# model.fit(features_train,labels_train)\n",
    "# pred_labels_train=model.predict(features_train_norm)\n",
    "# pred_labels_test=model.predict(features_test_norm)\n",
    "\n",
    "# print(\"Accuracy train: {}\".format(accuracy_score(labels_train,pred_labels_train)))\n",
    "# print(\"Accuracy test: {}\".format(accuracy_score(labels_test,pred_labels_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
