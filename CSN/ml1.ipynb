{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Study 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This self study sheet contains some code, and some questions. You should first walk through the code, and then work on the questions. The questions can be answered by extending this notebook with some additional code and text (\"markdown\") cells. A brief guide to formatting in markdown cells is here:<br>\n",
    "https://medium.com/ibm-data-science-experience/markdown-for-jupyter-notebooks-cheatsheet-386c05aeebed<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following just gives us some tools we'll need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmpl\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_iris\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from scipy.stats import multivariate_normal as mvn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A small utility we shall need later:\n",
    "def maxpos(A):\n",
    "    '''\n",
    "    Takes an n x k array A, and returns 1-dim n array where the i'th\n",
    "    entry is the index of column in A where the i'th row of A has its\n",
    "    maximal value (application: turns a probabilitiy distribution over\n",
    "    k classes for n instances into a single n-dimensional array of predictions)\n",
    "    '''\n",
    "    return np.argmax(A,axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the Iris dataset and inspect what it is about. Iris is probably the most heavily used dataset in machine learning. It even has its own Wikipedia page: https://en.wikipedia.org/wiki/Iris_flower_data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irisdata = load_iris()\n",
    "\n",
    "print(irisdata.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Iris data contains 4 features. In order to facillitate visualization of decision regions,\n",
    "it is better to use two features only, that then can be plotted in 2D. Here we select\n",
    "two features, and make a scatter plot of their values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat1 = 1\n",
    "feat2 = 2\n",
    "plt.scatter(irisdata.data[:,feat1],irisdata.data[:,feat2],c=irisdata.target)\n",
    "plt.xlabel(irisdata['feature_names'][feat1])\n",
    "plt.ylabel(irisdata['feature_names'][feat2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next define and train several classification models. We continue to only use the first two features of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the **K nearest neighbor** model. To define the model, we have to select the value of K, here called n_neighbors. The .fit function is the generic function for model training. For the K nearest neighbor model there is no actual training, however. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irisknn = KNeighborsClassifier(n_neighbors=3)\n",
    "irisknn.fit(irisdata.data[:,[feat1,feat2]],irisdata.target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following defines three different linear models: Linear regression is the same as 'Least squares regression on the slides of the first lecture; logistic regression and linear support vector machine will be studied in more detail in the following lectures. For now we need to know that for each of the models we first specify the desired model class, and then fit (or 'train', or 'learn') the model based on our labeled training data. The linear regression model requires the label information in a different format from the other two models, which is why we first construct the 'one hot encoding' representation. After the models have been fitted, they each define a linear decision boundary, and can classify unlabeled examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehotclasses = np.zeros((irisdata.target.size,3))\n",
    "for i in range(irisdata.target.size): \n",
    "    onehotclasses[i][irisdata.target[i]]=1.0\n",
    "  \n",
    "irislinreg = LinearRegression()\n",
    "irislinreg.fit(irisdata.data[:,[feat1,feat2]],onehotclasses)\n",
    "\n",
    "irislogreg = LogisticRegression()\n",
    "irislogreg.fit(irisdata.data[:,[feat1,feat2]],irisdata.target)\n",
    "\n",
    "irislinsvc = SVC(kernel='linear')\n",
    "irislinsvc.fit(irisdata.data[:,[feat1,feat2]],irisdata.target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting labeled data points and decision regions. First, a meshgrid is constructed consisting of a fine grid of points that we classify, and then use for visualization of the decision regions. Then, our classifier is applied to the grid (uncomment the pair of lines you want for either knn or one of the linear models).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxval1 = np.max(irisdata.data[:,feat1])\n",
    "maxval2 = np.max(irisdata.data[:,feat2])\n",
    "\n",
    "xx, yy = np.meshgrid(np.arange(0, maxval1+1, 0.02), np.arange(0, maxval2+1, 0.02))\n",
    "\n",
    "Z = irisknn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "meshclasses = Z\n",
    "\n",
    "#Z = irislinreg.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "#meshclasses = maxpos(Z)\n",
    "\n",
    "#Z = irislogreg.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "#meshclasses = Z\n",
    "\n",
    "#Z = irislinsvc.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "#meshclasses = Z\n",
    "\n",
    "\n",
    "meshclasses = meshclasses.reshape(xx.shape)\n",
    "plt.contourf(xx,yy,meshclasses,[-0.5,0.5,1.5,2.5],colors=('tomato','lightblue','lightgreen'))\n",
    "plt.scatter(irisdata.data[:,feat1],irisdata.data[:,feat2],c=irisdata.target, cmap = mpl.colors.ListedColormap(['r', 'b', 'g']))\n",
    "plt.xlabel(irisdata['feature_names'][feat1])\n",
    "plt.ylabel(irisdata['feature_names'][feat2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1** Visualize and describe the decision boundaries obtained from the KNN classifier and the linear regression classifier. Use different values of K in the KNN classifier, and also consider different selections of predictive features. This does not require any extension of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2:** Divide the data into 70% training and 30% test data. Learn KNN and linear regression classifiers from the training data, and evaluate their accuracy both on the training and test data. For the KNN classifier,\n",
    "draw the curves that show train/test accuracy as a function of K. Useful sklearn functions:<br>\n",
    "`sklearn.model selection.train test split`<br>\n",
    "`sklearn.metrics.accuracy score`<br>\n",
    "`sklearn.metrics.confusion matrix`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3:**  Also perform (some) of the previous experiments using the full set of 4 predictive features. How does that\n",
    "change the classification performance? Which of the 3 Iris classes is easiest to classify? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we revisit Exercise 2 from the first exercise sheet. We construct a dataset that is linearly separable, but may invite overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss1= mvn(mean=(-2,-2),cov=((1,0),(0,1))).rvs(random_state=1,size=100)\n",
    "gauss2= mvn(mean=(3.5,0),cov=((0.1,0),(0,0.1))).rvs(random_state=1,size=100)\n",
    "X=np.vstack([gauss1,gauss2])\n",
    "X=np.vstack([X,(0.5,-1)])\n",
    "Y=np.hstack([np.zeros(100),np.ones(101)])\n",
    "\n",
    "plt.scatter(X[:,0],X[:,1],c=Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare a meshgrid for the visualization of decision regions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leftx=np.min(X[:,0])-0.5\n",
    "rightx=np.max(X[:,0])+0.5\n",
    "lefty=np.min(X[:,1])-0.5\n",
    "righty=np.max(X[:,1])+0.5\n",
    "\n",
    "xx, yy = np.meshgrid(np.arange(leftx, rightx, 0.02), np.arange(lefty, righty, 0.02))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4**: What classifiers can you construct that appear to overfit by classifying the yellow outlier point as belonging to the yellow class? What other classifiers classify this point as purple?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1 (main, Dec 23 2022, 09:40:27) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "1a1af0ee75eeea9e2e1ee996c87e7a2b11a0bebd85af04bb136d915cefc0abce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
