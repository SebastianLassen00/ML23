{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Gaussian Model with BBVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Generate data from a simple model: Normal(10, 1)\n",
    "data = np.random.normal(loc = 10, scale = 1, size = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual estimation of the gradient of the ELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient estimator using sampling -- vanilla BBVI\n",
    "# We here assume the model X ~ Normal(mu, 1)\n",
    "# with unknown mu, that in itself is Normal, mean 0 and standard deviation 1000, \n",
    "# so effectively an uniformed prior. \n",
    "# The variational dstribution for mu is also Normal, with parameter q_mu_lambda\n",
    "# -- taking the role of lambda in the calculations -- and variance 1.\n",
    "\n",
    "def grad_estimate(q_mu_lambda, samples = 1):\n",
    "    # sum_grad_estimate will hold the sum as we move along over the <samples> samples. \n",
    "    sum_grad_estimate = 0\n",
    "    for i in range(samples):\n",
    "        # Sample one example from current best guess for the variational distribution\n",
    "        mu_sample = np.random.normal(loc=q_mu_lambda, scale=1, size=1)\n",
    "        \n",
    "        # Now we want to calculate the contribution from this sample, namely \n",
    "        # [log p(x, mu_sample) - log q(mu|lambda) ] * grad( log q(mu_sample|lambda) )\n",
    "        #\n",
    "        # First log p(x|mu_sample) + log p(mu_sample) - log q(mu_sample|lambda) \n",
    "        value = np.sum(norm.logpdf(data, loc=mu_sample, scale=1)) \n",
    "        + norm.logpdf(mu_sample, loc = 0, scale = 1000)  \n",
    "        - norm.logpdf(mu_sample, loc= q_mu_lambda, scale = 1)\n",
    "        \n",
    "        # Next grad (log q(mu_sample|lambda))\n",
    "        # The Normal distribution gives the score function with known variance as <value> - <mean>\n",
    "        grad_q = mu_sample - q_mu_lambda\n",
    "        \n",
    "        # grad ELBO for this sample is therefore in total given by\n",
    "        sum_grad_estimate = sum_grad_estimate + grad_q * value\n",
    "        \n",
    "    # Divide by number of samples to get average value -- the estimated expectation  \n",
    "    return sum_grad_estimate/samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the variation in gradient estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check the variation / \"unreliability\" of the gradient estimate we repeat \n",
    "# several times for the same lambda value and notice difference\n",
    "\n",
    "# Location to check -- close to the data mean (at +10). \n",
    "# The prior will move the variational optimium **slightly** away from the data mean, \n",
    "# but due to the large prior variance of mu this should be a very limited effect.\n",
    "# We should therefore expect a positive derivative (since we want to move \n",
    "# q_mu_lambda towards the data mean, that is, **increase** it)\n",
    "q_mu_lambda = 9\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.set()\n",
    "# Do with different sample sizes\n",
    "for sample_count in [1, 2, 3, 4, 5, 10, 25, 250]:\n",
    "\n",
    "    #loop\n",
    "    q_grad = []\n",
    "    for t in range(500):\n",
    "        q_grad.append(grad_estimate(q_mu_lambda, samples=sample_count))\n",
    "    \n",
    "    sns.distplot(q_grad, hist=False, label=\"$M = {:d}$\".format(sample_count))\n",
    "    \n",
    "    # Report back\n",
    "    print(\"M = {:2d} sample(s) in BBVI -- Mean of gradient: {:7.3f}; Std.dev. of gradient: {:7.3f}\".format(\n",
    "        sample_count, np.mean(q_grad), np.std(q_grad)))\n",
    "\n",
    "plt.xlim([-500, 500])\n",
    "plt.show()      \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What can we say about the gradient estimate based on the plots above?\n",
    "* Implement and test your own gradient ascent learning algorithm starting from, say, q_mu_lambda=-10. Experiment with different learning rates, number of samples used for the gradient estimate, possibly supplemented with momentum.\n",
    "* Optional: Adapt the implementation above so that we can also learn the variance of the q distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_mu_lambda = -10\n",
    "lr = 0.001\n",
    "alpha = 0.5 # \"Forget rate\"\n",
    "momentum = 0\n",
    "\n",
    "for i in range(20):\n",
    "    grad = grad_estimate(q_mu_lambda, samples=250)\n",
    "    momentum = momentum * alpha + grad\n",
    "\n",
    "    q_mu_lambda += momentum * lr\n",
    "\n",
    "    print(\"grad: \" + str(grad))\n",
    "    print(\"momentum: \" + str(momentum))\n",
    "    print(\"q_mu_lambda: \" + str(q_mu_lambda))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
