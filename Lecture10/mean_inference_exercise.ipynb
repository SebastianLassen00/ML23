{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "import torch\n",
    "from pyro.optim import SGD, Adam\n",
    "import pyro.distributions as dist\n",
    "from torch.distributions import constraints\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import beta\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook we the simple generative model from Slide 18, which you also experimented with in the notebook *student_BBVI.ipynb*:\n",
    " * https://www.moodle.aau.dk/mod/resource/view.php?id=1049031\n",
    "\n",
    "In the previous notebook we derived the required gradients manually. Here we instead rely on differentiation functionality in Pyro, which, in turn is based n PyTorch."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model in plate notation\n",
    "\n",
    "<img src=\"mean_model.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model defined in Pyro\n",
    "\n",
    "Here we define the probabilistic model. Notice the close resemblance with the plate specification above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_model(data):\n",
    "\n",
    "    # Define the random variable mu having a noral distribution as prior\n",
    "    mu = pyro.sample(\"mu\", dist.Normal(0.0,1000.0))\n",
    "    # variance = pyro.sample(\"variance\", dist.Normal(1,0))\n",
    "\n",
    "    # and now the plate holding the observations. The number of observations are determined by the data set \n",
    "    # supplied to the function. \n",
    "    with pyro.plate(\"x_plate\"):\n",
    "        pyro.sample(f\"X\", dist.Normal(mu, 1.0), obs=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The variational distribution\n",
    "\n",
    "In Pyro the variational distribution is defined as a so-called guide. In this example our variational distribution is a beta distribution with parameters q_alpha and q_beta:\n",
    "\n",
    "$$\n",
    "q(\\mu)= \\mathit{Normal}(\\mu | q_{mu}, 1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_guide(data):\n",
    "\n",
    "    # We initialize the variational parameter to 0.0. \n",
    "    q_mu = pyro.param(\"q_mu\", torch.tensor(0.0))\n",
    "    q_variance = pyro.param(\"q_variance\", torch.tensor(1.1), constraint=constraints.positive)\n",
    "\n",
    "    # The name of the random variable of the variational distribution must match the name of the corresponding\n",
    "    # variable in the model exactly.\n",
    "    pyro.sample(\"mu\", dist.Normal(q_mu, q_variance))\n",
    "    # variance = pyro.sample(\"variance\", dist.Normal(2,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning\n",
    "\n",
    "Here we encapsulate the learning steps, relying on standard stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(data):\n",
    "\n",
    "    pyro.clear_param_store()\n",
    "\n",
    "    elbo = pyro.infer.Trace_ELBO()\n",
    "    svi = pyro.infer.SVI(model=mean_model,\n",
    "                         guide=mean_guide,\n",
    "                         optim=SGD({'lr':0.001}),\n",
    "                         loss=elbo)\n",
    "\n",
    "    num_steps = 1000\n",
    "    for step in range(num_steps):\n",
    "        loss = svi.step(data)\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            print(f\"Loss for iteration {step}: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(np.random.normal(loc=100.0, scale=5.0, size=100),dtype=torch.float)\n",
    "learn(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the learned variational parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The learned parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmu = pyro.param(\"q_mu\").item()\n",
    "q_variance = pyro.param(\"q_variance\").item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean of vaiational distribution: {qmu}\")\n",
    "print(f\"q_variance: {q_variance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "* Adapt the code above to accomodate a slight more rich variational distribution, where we also have a variational parameter for the standard deviation:\n",
    "$$\n",
    "q(\\mu)= \\mathit{Normal}(\\mu | q_{mu}, q_{std})\n",
    "$$\n",
    "* Experiment with different data sets and parameter values. Try visualizing the variational posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-2022",
   "language": "python",
   "name": "ml-2022"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
